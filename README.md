# Machine-Learning-final

Tο train_hh_gt.csv και το train_hh_features.csv περιέχουν και τα δύο πληροφορίες που θα χρειαστούν στο training dataset, οπότε γίνεται στην ένωσή τους βάση το household ID που έχουν και το survey ID στο οποίο ανήκουν.  
Εφαρμόζεται συμπλήρωση των κενών τιμών, scaling και one-hot encoding.
Το αρχείο train_hh_gt.csv συγκεκριμένα περιέχει τα targets και τα αρχεία train_hh_features.csv και test_hh_features.csv έχουν τις ίδες στήλες/μεταβλητές.

Για τα χαρκτηριστικά του train υπολογίστηκε πίνακας correlation όπως φαίνεται παρακάτω.
Γενικά δεν παρατηρούνται έντονες συσχετίσεις μεταξύ των χαρακτηριστικών, με εξαίρεση μεταβλητές που έχουν να κάνουν με δημογραφικά στοιχεία, όπως με το μέγεθος του νοικοκυριού και ο αριθμός των παιδιών, τα οποία φαίνεται να ξεχωρίζουν, καθώς έχουν πολύ πιο έντονα και θερμά χρώματα.
<img width="1920" height="1080" alt="Στιγμιότυπο οθόνης (1079)" src="https://github.com/user-attachments/assets/3027042c-4464-4cda-b76c-cd58774a933a" />

---

## Κώδικας και pipeline
Για όλα τα μοντέλα εφαρμόστηκε η ίδια διαδικασία αξιολόγησης. Χρησιμοποιήθηκε GroupKFold με ομαδοποίηση βάσει του survey_id, έτσι ώστε κάθε survey να εμφανίζεται αποκλειστικά είτε στο σύνολο εκπαίδευσης είτε στο σύνολο επικύρωσης. Με τον τρόπο αυτό προσομοιώνεται η γενίκευση του μοντέλου σε δεδομένα διαφορετικών ετών συλλογής.

Σε κάθε fold το μοντέλο εκπαιδεύεται στο training set και προβλέπει την ημερήσια κατά κεφαλήν κατανάλωση των νοικοκυριών στο validation set. Από τις προβλέψεις αυτές υπολογίζονται δύο κατηγορίες σφαλμάτων:  
(α) σφάλματα σε επίπεδο νοικοκυριού, με χρήση των μετρικών MAE, και στη συνέχεια  
(β) σφάλματα στους poverty rates.

Τα poverty rates προκύπτουν συγκρίνοντας την προβλεπόμενη κατανάλωση κάθε νοικοκυριού με μια σειρά δοσμένων από dataset csv κατωφλίων κατανάλωσης και εφαρμόζοντας τα αντίστοιχα βάρη των νοικοκυριών. Τα προβλεπόμενα poverty rates συγκρίνονται με τα πραγματικα και υπολογίζεται το Mean Absolute Error.

Η διαδικασία επαναλαμβάνεται για όλα τα folds και στο τέλος υπολογίζεται ο μέσος όρος των μετρικών. Για τα μοντέλα που διαθέτουν υπερπαραμέτρους (Random Forest, XGBoost και MLP), δοκιμάστηκαν διαφορετικές ρυθμίσεις και επιλέχθηκε εκείνη που ελαχιστοποιεί το σφάλμα στους poverty rates.

( Σχόλια και σύντομη επεξήγση για βήματα/ροή υπάρχουν και στον κώδικα.)

---

## Επιλογή και σύγκριση αλγορίθμων

Οι αλγόριθμοι που επιλέχθηκαν είναι οι Linear Regression, Random Forest, XGBoost.  
Ο λόγος για τον οποίο χρησιμοποιηθηκε η γραμμική παλινδρόμηση είναι γιατι εξεταζει γραμμικότητα σχεσεων και είναι ένας βασικός αλγόριθμος ο οποίος εξετάζεται για την περίπτωση που θα μπορούσε να λύσει το πρόβλημα γραμμικά (αν και σε τόσο περίπλοκα προβλήματα αναμένεται να δυσκολευτεί καθώς λογικά γραμμικές σχέσεις δεν αρκούν για την πολυπλοκότητα του προβλήματος). Τα Random Forest, XGBoost και MLP επιλέχθηκαν γιατί μπορούν να αναπαραστήσουν μη γραμμικές σχέσεις και οι δύο είναι αλγόριθμοι δέντρων με διφορές στην υλοποίηση τους όπως θα δούμε παρκάτω και το MLP είναι αλγόριθμος βαθιάς μάθησης.

Τα αποτελέσματα δείχνουν καλύτερη απόδοση όσο αυξάνεται η πολυπλοκότητα του μοντέλου, με το MLP να έχει την υψηλότερη.
Από άποψη χρόνου εκτέλεσης ο XGBoost ήταν ο πιο χρονοβόρος, μετά το Random Forest, αναλόγως και των τιμών που δίνοταν στο tuning, γενικότερα όσο αυξανόταν πχ το βλαθος του δέντορυ, τόσο αυξανόταν και ο χρόνος. Μετά ήταν το MLP και πιο γρήγορο ήταν το Linear Regression.

---

## Περιορισμοί και δυνατά σημεία των μοντέλων

Όπως αναφέρθηκε και όπως φαίνεται η γραμμική παλινδρόμηση δεν καταφερνει να ανταπεξέλθει στις περίπλοκες σχεσεις των δεδομένων χρησιμοποιώντας γραμμικές σχέσεις. Από την άλλη, τα Random Forest, XGBoost (trees) και το MLP είναι πιο ευέλικτα, αλλά απαιτούν περισσότερo χρόνο εκπαίδεσης.
Παρκάτω παρουσιάζονται πιο αναλυτικά.

## Linear Regression: δεν συλλαμβάνει μη γραμμικές σχέσεις.
Είναι απλό και ερμηνεύσιμο μοντέλο, ωστόσω δεν μοντελοποιεί μη γραμμικές σχέσεις με αποτέλεσμα να επηρρεάζεται ημαντικά η απόδοσή του και ττέλος υποθέτει ότι τα δεδομένα είναι ανεξάρτητα μεταξύ τους, κάτι που συχνά δεν σχύει. Για αυτό και αναμένεται να παρουσιάσει χαμηλή απόδοση σε σύνθετα πραγματικά δεδομένα.

## Random Forest:
Το Random Forest βασίζεται στο bagging, όπου κάθε δέντρο εκπαιδεύεται σε διαφορετικό τυχαίο υποσύνολο των αρχικών δεδομένων  και χαρακτηριστικών (κάθε υποσύνολο έχει ίδιο πλήθος δεδομένων με το αρχικό). 
Με αυτόν τον τρόπο, αυξάνεται η ποικιλία των δειγμάτων που δίνονται στα επιμέρους δέντρα και το Random Forest είναι πιο ανθεκτικό στο overfitting, σε σχέση με ένα μεμονωμένο δέντρο απόφασης, ενώ επίσης έτσι μπορεί και μαθαίνει μη γραμμικές και άρα πιο σύνθετες σχέσεις. Επιπλέον μπορεί να εντοπίσει και να βγάλει σαν αποτέλεσμα ποιο χαρακτηριστικό έιναι πιο σημαντικό (feature importance).
Ωστόσο, ακριβώς λόγω αυξημένης πολυλπλοκότητας είναι λιγότερο ερμηνεύσιμο από το Linear Regression.

Το Random Forest αναμένεται να αποδώσει καλά σε σύνολα δεδομένων με μη γραμμικές σχέσεις (τα οποία εμφανίζονται στην πλειοψηφία των δεοδμένων του πραγματικού κοσμου, όπως και στο πρόβλημα της συγκεκριμένης άσκησης), με σύνθετες συσχετίσεις μεταξύ χαρακτηριστικών. Επίσης, χρειάζεται επαρκές πλήθος δεδομένων για εκπαίδευση και μπορεί να μην αποδώσει τόσο αποδοτικά σε πολύ μικρά σύνολα δεδομένων, καθώς εκεί το bagging δεν θα προσφέρει σημαντικό όφελος. 
Τέλος, λόγω της φύσης του ensemble, είναι λιγότερο ερμηνεύσιμο από απλούστερα μοντέλα, όπως η Linear Regression.

## XGBoost: 
ο XGBoost βασίζεται στον Gradient Boosting, ακολουθώντας παρόμοια λογικη εκπαίδευσης, αλλά με οριμένες βελτιώσεις. Κύρια επέκτασή του είναι η χρήση κανονικοποίησης (regularization), η οποία περιορίζει την πολυπλοκότητα του μοντέλου και συμβάλλει στη μείωση του overfitting. Επιπλέον, υποστηρίζει παράλληλη επεξεργασία, γεγονός που βοηθά την εκπαίδευσή του, καθώς και ενσωματωμένη διαχείριση ελλιπών τιμών χωρίς ανάγκη πρόσθετης προεπεξεργασίας.
Επιπλέον, εφαρμόζει κλάδεμα δέντρων βελτιώνοντας τη γενίκευση. Όλα τα παραπάνω το καθιστούν ιδιαίτερα ισχυρό μοντέλο για πολύπλοκα και μη γραμμικά δεδομένα.
Παρόλα αυτά όμως η αυξημένηππλυπλοκότητα, συνεπάγεται πιο πολλές υπερπαράμετρους και άρα δυσκολότερο (και πιο αργό) tuning.
Ενώ όπως και το Random Forest είναι δύσκολο στην ερμηνεία των αποτελεσμάτων του.
Τέλος, μπορεί να μην αποδώσει ικανοποιητικά όταν το διαθέσιμο σύνολο δεδομένων είναι μικρό, καθώς τότε υπάρχει αυξημένος κίνδυνος υπερπροσαρμογής.



## MLP: 
Το MLP έχει αυξημένη πολυπλοκότητα, λόγω της χρήσης νευρωνων και επιπέδων νευρωνων. Η αρχιτεκτονική του με πολλαπλούς νευρώνες και επίπεδα μπορεί να προσεγγίζει πολύπλοκες και μη γραμμικές σχέσεις στα δεδομένα και για αυτόν τον λόγο είναι αποδοτικό για πιο σύνθετα προβλήματα, όπου αναμλενονται μη γραμμικά μοτίβα μεταξύ των χαρακτηριστικών. Γενικά απόδοσή του βελτιώνεται όσο αυξάνεται το πλήθος των διαθέσιμων δεδομένων εκπαίδευσης, καθώς έτσι μπορεί να μάθει πιο αξιόπιστες αναπαραστάσεις χωρίς να υπερπροσαρμόζεται.

Ωστόσο, το MLP απαιτεί σωστή κανονικοποίηση των δεδομένων και επαρκές πλήθος δειγμάτων, καθώς σε αυτήν την περίπτωση υπάρχει αυξημένος κίνδυνος overfitting ή χαμηλής απόδοσης. Επιπλέον, διαθέτει μεγάλο αριθμό υπερπαραμέτρων, γεγονός που καθιστά τη διαδικασία ρύθμισης πιο απαιτητική και χρονοβόρα. Τέλος, λόγω της πολυπλοκότητάς του, είναι δύσκολα ερμηνεύσιμο, γεγονός που αποτελεί μειονέκτημα σε εφαρμογές όπου απαιτείται κατανόηση της συμβολής των χαρακτηριστικών, κάτι το οποίο το Random Forest για παράδειγμα παρέχει.

---
## Αποτλέσματα
Το Linear Regression όπως ήταν αναμενόμενο, παρουσιάζει τη χαμηλότερη συνολική απόδοση. 

Αμέσως μετά είναι το Random Forest, στο οποίο όμως παρατηρείται βελτίωση και στο Rates MAE και στο Household MAE, με το καλύτερο σύνολο παραμέτρων να είναι {'n_estimators': 200, 'max_depth': 20, 'min_samples_leaf': 5}, δείχνοντας ότι βαθύτερα δέντρα οδηγούν σε καλύτερη απόδοση.

Το XGBoost είναι το αμέσως επόμενο αλλά αξιζει να σημειωθεί ότι επιτυγχάνει την καλύτερη πρόβλεψη κατανάλωσης σε επίπεδο νοικοκυριού (Household MAE), κατι που αποδεικνύει ότι το boosting εκμεταλλεύεται καλύτερα πολύπλοκες αλληλεπιδράσεις μεταξύ των χαρακτηριστικών. 

Τέλος, το MLP (256,128) παρουσιάζει τη χαμηλότερη τιμή Rates MAE.
Αυτό θα πιλεχθεί στη συνέχεια για την τελική δοκιμή στα σύνολα επικυρωσης (test) καθ΄ςω στόχος είνια τα rates, στα οποία υψηλότερο σκορ έχει το MLP.
Παρατηρείται ότι ενώ το Household MAE του δεν είναι το μικρότερο, η συνολική του συμπεριφορά οδηγεί σε πιο αξιόπιστα ποσοστά φτώχειας.

Συνολικά, τα αποτελέσματα δείχνουν ότι τα μη γραμμικά μοντέλα (Random Forest, XGBoost, MLP) υπερέχουν του γραμμικού λόγω του ότι πρόκειται για πρόβλημα πραγματικού κόσμου με μη γραμμικά δδομένα.

<img width="1078" height="640" alt="Στιγμιότυπο οθόνης (1077)" src="https://github.com/user-attachments/assets/f8e54e67-126e-4d9a-8d68-1718c8cca3f9" />
<img width="507" height="289" alt="Στιγμιότυπο οθόνης (1078)" src="https://github.com/user-attachments/assets/11d951e8-06a2-4a41-b7dc-483218f782b3" />


---

### Θα βοηθούσε πιθανώς

- Προσθήκη περισσότερων ποιοτικών χαρακτηριστικών (π.χ. δημογραφικά, οικονομικοί δείκτες).  
- καθαρισμός και μείωση θορύβου (να μην εχει ελλιπείς τιμές).  
- Περισσότερα δείγματα για καλύτερη εκπαίδευση των μοντέλων.  
- Πιθανως αν ειχε και δεδομένα ανά survey σαν επιπλέον πληροφορία


---
Σε όλες τις προσπάθειες submit έβγαζε σφάλμα, οπότε σαν ένδειξη ότι έγνε προσπάθεια υποβολής δίνεται η παρακάτω εικόνα:
<img width="857" height="710" alt="Στιγμιότυπο οθόνης (1074)" src="https://github.com/user-attachments/assets/b6559098-35f8-4901-b907-d5c3ae9d15ff" />
